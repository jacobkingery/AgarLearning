<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
<!--     <title>Agarlearning by jacobkingery</title>
 -->    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
</head>
<body>
  	<section class="page-header">
      <h1 class="project-name">Agarlearning</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/jacobkingery/AgarLearning" class="btn">View on GitHub</a>
      <a href="index.html" class="btn">Home Page</a>
      <a href="projectProposal.html" class="btn">Project Proposal</a>
      <a href="blogPost1.html" class="btn">Blog Post 1</a>
    </section>

<!-- <title>Project Proposal</title>
 -->
<xmp  style="display:none;">
# Project Update 2 (A new direction)
**Jacob, Mac-I, and Sophia**

## We've Changed Our Project!
In our last blog post, we mentioned that we were planning on implementing reinforcement learning for an [Agar.io](agar.io) bot. Unfortunately, that didn't go as well as we hoped, and after attempting to implement reinforcement learning for a bot in the Agar implementation, we’ve decided to switch our project direction a little. We’ve done this for a few reasons that we’ll talk about below: 

First, we found the implementation of Agar a little complicated. Although we had successfully implemented Python/Node communication and were able to control a bot using a python script, we were unsuccessful in teaching the bot that going towards food was good. We have a few hypotheses about why this was happening including that we didn’t correctly use the reinforcement learning library we chose, or that the bot needed more training time to figure out what was happening.

Although this is something we likely could have figured out, we decided to re-scope our project a bit. This is mostly a result of us wanting to stay true to our learning goals with this project. We had two learning goals that were particularly important to us in this sense. The first of these goals was to appropriately scope the project. As we were trying to implement reinforcement learning with our Agar code, we quickly realized how complicated of a problem this is to solve. Additionally, it was difficult for us to see whether our bots were doing just a random walk (randomly choosing movements), or whether they were actually learning smarter behaviors. Although we certainly could have implemented our own set of visualization tools that interfaced with Agar/Ogar, we thought that this would not really have helped us to learn about neural networks. Since this was a learning goal that all three of us had, we decided not spend time to make a new set of visualization tools instead of learning more about neural networks. 

As a result, the combination of the problem being a very difficult problem to solve with reinforcement learning, and the time we would have spent to build tools that didn’t actually get us closer to our learning goals, we’ve decided to re-scope our project a bit.

As an added benefit, since we’re the ones implementing the game (more about that below!), we also know how to interface with every part of the code instead of trying to figure out how to interface with code that other people have written. 

## Our New Project/Setup
Our new game is inspired by Agar, but is more discretized. Its world is a grid, and there is food placed in some of the cells. The goal of the player is to eat all of the food in as few moves as possible. The allowed moves are to go up, right, down, or left by one cell. If the player is at the edge of the grid and tries to move outside of the grid, it stays in the same place.

```
2 0 1
0 0 1
1 0 0
```
In the grid representation above, the number 2 represents the player, and the three number 1s represent the food. If the player tried to move up or left, the gamestate would remain the same.

In building our new system, we created modular object-oriented components containing the game logic, reinforcement learning algorithm, and neural network implementation that all come together in a main game loop. In this loop, the current state of the game is retrieved from the game object and passed into a method of the reinforcement learning object which returns the next action to take. The reinforcement learning object uses the neural network object to predict the expected values of each move and then determines which move to take. After the game object is updated with the move, it returns the reward obtained by that action as well as the new state of the game. Currently, the player is given a reward of +1 for eating a food, 0 for moving without eating a food, and -1 for not moving (running into a wall). The state, action, reward, and new state are then used by the reinforcement learning object to train the neural network.

## Some initial Learning

![Cumulative winning losing and draw percentages over the number of games played](images/ComputerPerformance.png "Well, we did get better at playing tic-tac-toe...")

</xmp>

<script src="http://strapdownjs.com/v/0.2/strapdown.js"></script>
</html>
