<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
<!--     <title>Agarlearning by jacobkingery</title>
 -->    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
</head>
<body>
  	<section class="page-header">
      <h1 class="project-name">Agarlearning</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/jacobkingery/AgarLearning" class="btn">View on GitHub</a>
      <a href="index.html" class="btn">Home Page</a>
      <a href="projectProposal.html" class="btn">Project Proposal</a>
      <a href="blogPost1.html" class="btn">Blog Post 1</a>
    </section>

<!-- <title>Project Proposal</title>
 -->
<xmp  style="display:none;">
# Project Update 2 (A new direction)
**Jacob, Mac-I, and Sophia**

## We've Changed Our Project!
In our last blog post, we mentioned that we were planning on implementing reinforcement learning for an [Agar.io](agar.io) bot. Unfortunately, that didn't go as well as we hoped, and after attempting to implement reinforcement learning for a bot in the Agar implementation, we’ve decided to switch our project direction a little. We’ve done this for a few reasons that we’ll talk about below: 

First, we found the implementation of Agar a little complicated. Although we had successfully implemented Python/Node communication and were able to control a bot using a python script, we were unsuccessful in teaching the bot that going towards food was good. We have a few hypotheses about why this was happening including that we didn’t correctly use the reinforcement learning library we chose, or that the bot needed more training time to figure out what was happening.

Although this is something we likely could have figured out, we decided to re-scope our project a bit. This is mostly a result of us wanting to stay true to our learning goals with this project. We had two learning goals that were particularly important to us in this sense. The first of these goals was to appropriately scope the project. As we were trying to implement reinforcement learning with our Agar code, we quickly realized how complicated of a problem this is to solve. Additionally, it was difficult for us to see whether our bots were doing just a random walk (randomly choosing movements), or whether they were actually learning smarter behaviors. Although we certainly could have implemented our own set of visualization tools that interfaced with Agar/Ogar, we thought that this would not really have helped us to learn about neural networks. Since this was a learning goal that all three of us had, we decided not spend time to make a new set of visualization tools instead of learning more about neural networks. 

As a result, the combination of the problem being a very difficult problem to solve with reinforcement learning, and the time we would have spent to build tools that didn’t actually get us closer to our learning goals, we’ve decided to re-scope our project a bit.

As an added benefit, since we’re the ones implementing the game (more about that below!), we also know how to interface with every part of the code instead of trying to figure out how to interface with code that other people have written. 

## Our New Project/Setup
Our new game is inspired by Agar, but is more discretized. Its world is a grid, and there is food placed in some of the cells. The goal of the player is to eat all of the food in as few moves as possible. The allowed moves are to go up, right, down, or left by one cell. If the player is at the edge of the grid and tries to move outside of the grid, it stays in the same place.

```
2 0 1
0 0 1
1 0 0
```
In the grid representation above, the number 2 represents the player, and the three number 1s represent the food. If the player tried to move up or left, the gamestate would remain the same.

In building our new system, we created modular object-oriented components containing the game logic, reinforcement learning algorithm, and neural network implementation that all come together in a main game loop. In this loop, the current state of the game is retrieved from the game object and passed into a method of the reinforcement learning object which returns the next action to take. The reinforcement learning object uses the neural network object to predict the expected values of each move and then determines which move to take. After the game object is updated with the move, it returns the reward obtained by that action as well as the new state of the game. Currently, the player is given a reward of +1 for eating a food, 0 for moving without eating a food, and -1 for not moving (running into a wall). The state, action, reward, and new state are then used by the reinforcement learning object to train the neural network.

## Some initial Learning
Our neaural network currently uses 1 hidden layer and has 2 modes that determine the configuration of input and output nodes. In mode 0, the input to the neural network is simply the game state and the output consists of 4 nodes, each corresponding to a different action (up, down, left, right). In mode 1, the input is the state of the game as well as the possible actions one-hot encoded. The output is a single node that corresponds the value of the particular move that was encoded in the input. 
Our reinforcement algorithm is the classic [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) method. 

To start off we decided to look at the difference between mode 1 and mode 0 and how they performed when faced with the same intial state versus different intial states. 
[](images/sameGameEveryTimeMode0.png "")
In the above graph of the bot in mode 0 with the same board every game, we see that number of moves it takes the bot to eat all the food qucikly decreases to arounf 6 moves (The best possible). However, every 10-50 games the number of moves spikes dramatically. At this time we are unsure what is causing this but we hypothesize that exploratory behavior may be throwing the neural net off. Even if a terrible exploratory move was made we not expect this throw the bot off so badly. 

[](images/sameGameEveryTimeMode1.png "")

In the above graph of the bot in mode 1 with the same board every game, we see simliar early convergence behavior as mode 0. However, after that the bot begins to perform very poorly with only small areas on convergence. 

[](images/differentGameEveryTimeMode0.png "")
In the above graph of the bot in mode 0 with a different board every game, we see a slower decrease in the number of moves it takes to win but it does nevertheless eventually get down to an average of around 13 moves. 

[](images/differentGameEveryTimeMode1.png "")
In the above graph of the bot in mode 1 with a different board every game, we see that the bot never really improves and argueably gets worse.

Based on what we learned we will likely proceed using mode 0 for future iterations.

</xmp>

<script src="http://strapdownjs.com/v/0.2/strapdown.js"></script>
</html>
