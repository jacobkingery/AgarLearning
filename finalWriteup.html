<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
<!--     <title>Agarlearning by jacobkingery</title>
 -->    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen"><script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

</head>
<body>
  	<section class="page-header">
      <h1 class="project-name">Agarlearning</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/jacobkingery/AgarLearning" class="btn">View on GitHub</a>
      <a href="index.html" class="btn">Home Page</a>
      <a href="projectProposal.html" class="btn">Project Proposal</a>
      <a href="blogPost1.html" class="btn">Blog Post 1</a>
      <a href="blogPost2.html" class="btn">Blog Post 2</a>
    </section>

<!-- <title>Project Proposal</title>
 -->
<xmp  style="display:none;">
# Final Writeup
**Jacob, Mac-I, and Sophia**

## Project Background
For our final project for our data science course, we were interested in learning about reinforcement learning. Specifically, we were interested in teaching a computer how to play a game. The game we chose to learn how to play was a grid-based game that had a player move around the grid until they ate all of the food on the grid (we'll talk more about this game later!) To do this, we used a machine learning method called reinforcement learning. 

### What is Reinforcement Learning?

In most of the machine learning we’ve done in this course up until now, we were trying to learn a model that would allow us to predict the correct output given a set of previously collected inputs. While this is great if you already have a lot of data collected, this wasn’t the case for us. Since we were trying to teach a computer how to do something, in our case play a game, we didn’t have a dataset readily collected. As a result, we had two options. First, we could collect a dataset of us, or other people, playing a game, and teach the computer how to play a game. Our other option, reinforcement learning, was to let a computer teach itself how to play a game by seeing which actions have positive results and which have negative consequences. For a more in-depth explanation of a simple implementation of an example of reinforcement learning, please take a look at our [blog post!](http://jacobkingery.github.io/AgarLearning/blogPost1.html)

#### Some important vocabulary

Before we dive into the specifics of our exact problem setup and reinforcement learning implementation, let’s define some vocabulary! (These definitions are taken from our first blog post.)

**State (s):** We define the state as everything the computer currently knows about what is happening. 

**Action (a):** Although this is fairly self-explanatory,an action is anything that a computer can do when the game is in a certain state.

**Policy (π):** Our policy is a function that defines how the computer behaves at any given point in time.

**Reward function:** The reward function tells the computer, that, given a certain state-action pair the reward for performing that action, given the current state. This reward is determined based on how desirable the result of performing a given action from a given state is. The reward of transitioning from state s to state s' given action a is normally written as r(s,a,s')

**Value function:** Although some state, action pairs might have a good immediate outcome, they might not be as desirable in the long-term. The value function seeks to predict the long-term desirability of a state action pair. The predicted value of a given state under policy π, is written as vπ(s), while the value of state s and taking action a is written as qπ(s,a).


Although we used a type of reinforcement learning called Q-learning in our project, there are other approaches for reinforcement learning. For more information about other approaches to reinforcement learning and more information about reinforcement learning in general, check out one of our favorite resources [“Reinforcement Learning: An Introduction", by Richard Sutton and Andrew Barto](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html).

## Our initial Project
For this project, we started out trying to create a bot for the online game [agar.io](agar.io). In this game, players are cells moving around an area trying to eat food other other players that are smaller than they are. To build our bot, we found an open-source implementation of the agar game, called [ogar](https://github.com/OgarProject/Ogar). In this node.js implementation we could control everything from the size of the playing area to the number of food that was in the playing area. We figured that it was best to start our reinforcement learning adventure by starting simple, so we created a relatively small board. Then we set about implementing our bot. This involved having the node server communicate with a python script that would do the reinforcement learning. Although we got the node-python communication working fairly quickly, and started work trying out a few reinforcement learning techniques (mostly least squares policy iteration) we eventually realized that there were a few problems with this setup: 

**Reinforcement learning in a continuous space is difficult.** Since this game is played on pretty much a continuous area the state space is continuous, rather than being played on a grid. This means that there is a very large state space. What was more of a problem, though, was the action space was also continuous. In [agar.io][agar.io], players control their cells using the mouse, and the bot moves in the direction of the mouse. This means that in the case of our learning bot, an action would effectively be setting the mouse position of the "player" controlling the bot. A continuous action space is not something that is a solved problem in the reinforcement learning field, so we didn't think it was appropriately scoped project, for our first adventure with reinforcement learning.

**It was hard to see whether the bot was getting better.**  Although we had started to implement some reinforcement learning algorithms, we found that we had a lot of trouble figuring out if the bot was learning, or whether the bot just happened to be getting lucky doing a random walk. (A random walk is the bot moving across the board in a random way.) Solving this problem was particularly difficult because the ogar implementation interfaces with the [agar.io](agar.io) frontend, so we would basically have had to build our own frontend to get around this lack of information about how well our bot was learning.  

Although we could have certainly figured out ways to deal with these challenges, we were more interested in exploring how reinforcement learning and neural networks can be used together than we were in solving the particular problem of building the world's best [agar.io](agar.io) bot. As a result, we decided to simplify our game a little.

## Our Simpler Game
Due to the struggles we had with our Agar.io reinforcement learning approach, we developed our own, simpler game. This game consisted of a gridded board of variable rectangular dimensions. Each square could contain a food, the bot, or nothing. We encoded this information as a numpy matrix, where -1 meant that there was nothing in that cell, 0 meant that our bot was there, and 1 meant that there was food there. For example, one possible 3 by 3 game board looks like this

```
 0 -1 -1
-1  1 -1
-1 -1 -1
```

In this case, the bot is at position 0,0, and there is a food at 1,1, while all the other cells are empty. In our game, the bot has four possible actions: move left, right, up, or down. If the bot tries to move into a wall, it simply does not move. The game ends when the bot has eaten all the food on the board. 

*For more information about the implementation of our game, please consult the file GridLearning/game.py*

### Our Reinforcement Learning Setup
After simplifying our game, we also implemented our reinforcement learning setup. 

In our case we used a relatively simple reinforcement learning method called Q-learning. This is a reinforcement learning method that learns the value, or Q, function. The computer uses the Q function to figure out which moves are the most advantageous. While in some simple cases, the Q function can just be a lookup table. In our case, however, we're using a neural network to learn the value function.

If we were to use a table of state, action pairs mapping to a value, we would initialize our starting Q function to have some starting values. (For an example of using a table as the Q function, please take a look at our [first blog post](blogPost1.html)!)

Then, we would play a bunch of games, and iteratively update our table so that the Q function more accurately reflects the actual value function of the game. 

In our particular implementation, however, we're using a neural network to approximate the Q function. Before we dive too deep into how exactly this works, let's talk a bit about what a neural network actually is. 

#### What is a Neural Network?
THAT DEFINTION GOES HERE!!

Okay, now that we've got an understanding about neural networks, let's talk about how exactly we're using them in our reinforcement learning setup. In our case, the input to the neural network is the state of the game, and we have four outputs; one for each action. The outputs correspond to the predicted value of performing that action based on the current state. 

The information we're using to learn the Q function, in other words our training data, is a state and then a set of four values for each action. We collect, and update, this every time we make a move in our game. 

For the action we actually took from the state we were in, we update the predicted value of that move using the following equation:


$$Q_{t+1}(s_t,a_t) = Q_t(s_t, a_t) + \alpha_t(s_t,a_t)\times$$

$$(R_{t+1} + \gamma \times max(Q_t(s_t+1,a))) - Q_t(s_t, a_t))$$

In other words, we adjust the value of the current state by our learning rate $\alpha_t$ multiplied by the sum of the reward, $R_{t+1}$, our discount factor, $\gamma$ times the predicted value of the best action from the new state, $max(Q_t(s_t+1,a)))$, and the negative old value $ Q_t(s_t, a_t)$. In other words, we bring the new value of the current state some amount closer to the value of the next state based on the equation above.

We then feed this into the neural network to help it learn a better Q function. 

*For more information on exactly how we implemented the reinforcement learning and the neural network, please consult GridLearning/rl.py and GridLearning/nn.py, respectively.*

### Putting it all together
To combine the game, our reinforcement learning algorithm, and the neural network to do actual learning, we created a main loop (GridLearning/mainLoop.py) to handle us playing tons of games and having the computer learn from them. Our system diagram for this can be seen below:

![](images/System_Diagram.png "")

In this system diagram, we have color coded where in our code each part happens. The left-most column are all the steps that we go through. For steps with additional rectangles to the right, these rectangles provide more information based on how we're going about performing those steps. 


## Our Explorations

* Learning to play 1 game (Story: We could learn to play any game, but we’re going to use games that take the least time to learn -- 3x3, 1 food)
  * sameGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training
  * sameGame-4x4_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training
* Different Game every n games (Story: nice stepping stone  -- we can learn multiple games. This is fairly short. -- if this winds up not being interesting, let’s delete this)
  * ![](images/differentGameEvery50-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")
  * ![](images/differentGameEvery50-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
* Different Game every time (Story: took it to the next level; it took longer but it did learn)
  * ![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")
  * ![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
* Focus on different game every time
* Play with exploratory rate
  * Greedy-e
    * Constant vs decay
    
      ![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")

      _The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2._

      ![](images/differentGame-3x3_1-constDF_0.5-greedy-decayER_0.5_5000-constLR_0.9-eval.png "")

       _The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate decaying from 0.5 with a time constant of 5000 games._

      Using a fixed discount factor and learning rate, the best constant exploratory rate we found was 0.2. This means that each time we make a move (unless we're evaluating), there is a 20% chance of taking a random action instead of choosing the action with the greatest value predicted by the neural network.
      We also looked at decreasing the exploratory rate each game using an exponential decay function.  The best parameters we found for this were a starting value of 0.5 and a time constant of 5000 games.  This appears to be better than the constant exploratory rate; it appears to be learning optimal play faster using the decaying exploratory rate.

  * Boltzmann
    * Constant vs decay
      * ![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.9-eval.png "")
      * ![](images/differentGame-3x3_1-constDF_0.5-Boltz-decayER_0.2_2000-constLR_0.9-eval.png "") We got very lucky, both .19 and .21 hung
  * summary/conclusions
    * Overall spikes are lower with boltzmann
      * ![](images/differentGame-3x3_1-constDF_0.5-greedy-decayER_0.5_5000-constLR_0.9-training.png "")
      * ![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.9-training.png "")
* Play with learning rate
  * Constant vs decay
    * ![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.1-eval.png "")
    * ![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-decayLR_0.4_2000-eval.png "")
* Other game spaces (Multiple food)
  * differentGame-3x3_3-constDF_0.5-Boltz-decayER_0.2_1000-decayLR_0.9_1000-eval



</xmp>

<script src="http://strapdownjs.com/v/0.2/strapdown.js?mathjax=y"></script>
</html>
