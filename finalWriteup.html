<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
<!--     <title>Agarlearning by jacobkingery</title>
 -->    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">

</head>
<body>
  	<section class="page-header">
      <h1 class="project-name">Agarlearning</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/jacobkingery/AgarLearning" class="btn">View on GitHub</a>
      <a href="index.html" class="btn">Home Page</a>
      <a href="projectProposal.html" class="btn">Project Proposal</a>
      <a href="blogPost1.html" class="btn">Blog Post 1</a>
      <a href="blogPost2.html" class="btn">Blog Post 2</a>
    </section>

<!-- <title>Project Proposal</title>
 -->
<xmp  style="display:none;">
# Final Writeup
**Jacob, Mac-I, and Sophia**

## Project Background
For our final project for our data science course, we were interested in learning about reinforcement learning. Specifically, we were interested in teaching a computer how to play a game. The game we chose to learn how to play was a grid-based game that had a player move around the grid until they ate all of the food on the grid (we'll talk more about this game later!) To do this, we used a machine learning method called reinforcement learning. 

### What is Reinforcement Learning?

In most of the machine learning we’ve done in this course up until now, we were trying to learn a model that would allow us to predict the correct output given a set of previously collected inputs. While this is great if you already have a lot of data collected, this wasn't the case for us. Since we were trying to teach a computer how to do something, in our case play a game, we didn't have a dataset readily collected. As a result, we had two options. First, we could collect a dataset of us, or other people, playing a game, and teach the computer how to play a game by learning how we played the game. Our other option, reinforcement learning, was to let a computer teach itself how to play a game by seeing which actions have positive results and which have negative consequences. For a more in-depth explanation of a simple implementation of an example of reinforcement learning, please take a look at our [first blog post!](http://jacobkingery.github.io/AgarLearning/blogPost1.html)

#### Some Important Vocabulary

Before we dive into the specifics of our exact problem setup and reinforcement learning implementation, let’s define some vocabulary! (These definitions are taken from our first blog post.)

**State ($s$):** We define the state as everything the computer currently knows about what is happening. 

**Action ($a$):** Although this is fairly self-explanatory,an action is anything that a computer can do when the game is in a certain state.

**Policy ($\pi$):** Our policy is a function that defines how the computer behaves at any given point in time.

**Reward function:** The reward function tells the computer, that, given a certain state-action pair the reward for performing that action, given the current state. This reward is determined based on how desirable the result of performing a given action from a given state is. The reward of transitioning from state s to state $s'$ given action $a$ is normally written as $r(s,a,s')$

**Value function:** Although some state, action pairs might have a good immediate outcome, they might not be as desirable in the long-term. The value function seeks to predict the long-term desirability of a state action pair. The predicted value of a given state under policy $\pi$, is written as $v_π(s)$, while the value of state s and taking action a is written as $q_π(s,a)$.

Although we used a type of reinforcement learning called Q-learning in our project, there are other approaches for reinforcement learning. For more information about other approaches to reinforcement learning and more information about reinforcement learning in general, check out one of our favorite resources: [“Reinforcement Learning: An Introduction", by Richard Sutton and Andrew Barto](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html).

## Our Initial Project
For this project, we started out trying to create a bot for the online game [agar.io](agar.io). In this game, players are cells moving around an area trying to eat food other other players that are smaller than they are. To build our bot, we found an open-source implementation of the agar game, called [ogar](https://github.com/OgarProject/Ogar). In this node.js implementation we could control everything from the size of the playing area to the number of food that was in the playing area. We figured that it was best to start our reinforcement learning adventure by starting simple, so we created a relatively small board. Then we set about implementing our bot. This involved having the node server communicate with a python script that would do the reinforcement learning. Although we got the node-python communication working fairly quickly, and started work trying out a few reinforcement learning techniques (mostly least squares policy iteration) we eventually realized that there were a few problems with this setup: 

**Reinforcement learning in a continuous space is difficult.** Since this game is played on pretty much a continuous area the state space is continuous, rather than being played on a grid. This means that there is a very large state space. What was more of a problem, though, was the action space was also continuous. In [agar.io](agar.io), players control their cells using the mouse, and the bot moves in the direction of the mouse. This means that in the case of our learning bot, an action would effectively be setting the mouse position of the "player" controlling the bot. A continuous action space is not something that is a solved problem in the reinforcement learning field, so we didn't think it was appropriately scoped project, for our first adventure with reinforcement learning.

**It was hard to see whether the bot was getting better.**  Although we had started to implement some reinforcement learning algorithms, we found that we had a lot of trouble figuring out if the bot was learning, or whether the bot just happened to be getting lucky doing a random walk. (A random walk is the bot moving across the board in a random way.) Solving this problem was particularly difficult because the ogar implementation interfaces with the [agar.io](agar.io) front-end, so we would basically have had to build our own front-end to get around this lack of information about how well our bot was learning.  

Although we could have certainly figured out ways to deal with these challenges, we were more interested in exploring how reinforcement learning and neural networks can be used together than we were in solving the particular problem of building the world's best [agar.io](agar.io) bot. As a result, we decided to simplify our game a little.

## Our Simpler Game
Due to the struggles we had with our Agar.io reinforcement learning approach, we developed our own, simpler game. This game consisted of a gridded board of variable rectangular dimensions. Each square could contain a food, the bot, or nothing. We encoded this information as a numpy matrix, where -1 meant that there was nothing in that cell, 0 meant that our bot was there, and 1 meant that there was food there. For example, one possible 3 by 3 game board looks like this

``` python
 0 -1 -1
-1  1 -1
-1 -1 -1
```

In this case, the bot is at position (0,0), and there is a food at (1,1), while all the other cells are empty. In our game, the bot has four possible actions: move left, right, up, or down. If the bot tries to move into a wall, it simply does not move. The game ends when the bot has eaten all the food on the board. 

*to see how we implemented the game, please consult the file GridLearning/game.py*

### Our Reinforcement Learning Setup
After simplifying our game, we also implemented our reinforcement learning setup. 

In our case we used a relatively simple reinforcement learning method called Q-learning. This is a reinforcement learning method that learns the value, or Q, function. The computer uses the Q function to figure out which moves are the most advantageous. While in some simple cases, the Q function can just be a lookup table. In our case, however, we're using a neural network to learn the value function.

If we were to use a table of state, action pairs mapping to a value, we would initialize our starting Q function to have some starting values. (For an example of using a table as the Q function, please take a look at our [first blog post](blogPost1.html)!)

Then, we would play a bunch of games, and iteratively update our table so that the Q function more accurately reflects the actual value function of the game. 

In our particular implementation, however, we're using a neural network to approximate the Q function. Before we dive too deep into how exactly this works, let's talk a bit about what a neural network actually is. 

#### What is a Neural Network?
A neural network is a machine learning model that is inspired by biological nervous systems. It is composed of layers of neurons that are connected as a network. The image below shows a very simple neural network with three inputs and two outputs. The layer of neurons in the middle is referred to as a 'hidden layer', and there can be any number of hidden layers in a neural network.

![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg "")

_Neural network with three inputs, one hidden layer with four neurons, and two outputs; image from https://en.wikipedia.org/wiki/File:Colored_neural_network.svg_

Each neuron takes in each of the outputs of the previous layer, multiplies them by weights, adds a bias, and passes them through an activation function (such as a [hyperbolic tangent](http://mathworld.wolfram.com/HyperbolicTangent.html) or [sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html)) to each of the neurons in the next layer. The number of hidden layers, number of neurons in each layer, and activation functions between the layers are all set by the designer of the neural network (in this case, us!).

Like other machine learning models, neural networks need to be trained. By comparing the outputs of the neural network to 'correct' results, the error can be minimized using an optimization technique such as gradient descent.

Now that we've got an understanding about neural networks, let's talk about how exactly we're using them in our reinforcement learning setup. In our case, the input to the neural network is the state of the game, and we have four outputs; one for each action. The outputs correspond to the predicted value of performing that action based on the current state. If you remember our description of reinforcement learning above, you'll remember that we said that the inputs of the value function are the state and the action. We've deviated a little from that here in that our inputs are the state and our outputs our four values, one for each action. For whatever reason, we've found that this appears to help the computer learn our game faster. (To see examples, check out [blog post 2](blogPost2.html)!)

The information we're using to learn the Q function, in other words our training data, is a state and then a set of four values for each action. We collect, and update, this every time we make a move in our game. 

For the action we actually took from the state we were in, we update the predicted value of that move using the following equation:

$$Q_{t+1}(s_t,a_t) = Q_t(s_t, a_t) + \alpha_t(s_t,a_t)\times (R_{t+1} + \gamma \times max(Q_t(s_t+1,a)) - Q_t(s_t, a_t))$$

In other words, we adjust the value of the current state by our learning rate $\alpha_t$ multiplied by the sum of the reward, $R_{t+1}$, our discount factor, $\gamma$ times the predicted value of the best action from the new state, $max(Q_t(s_t+1,a))$, and the negative old value $ Q_t(s_t, a_t)$. In other words, we bring the new value of the current state some amount closer to the value of the next state based on the equation above.

We then feed this into the neural network to help it learn a better Q function. 

*For more information on exactly how we implemented the reinforcement learning and the neural network, please consult GridLearning/rl.py and GridLearning/nn.py, respectively.*

### Putting it all together
To combine the game, our reinforcement learning algorithm, and the neural network to do actual learning, we created a main loop (GridLearning/mainLoop.py) to handle us playing tons of games and having the computer learn from them. Our system diagram for this can be seen below:

![](images/System_Diagram.jpg "")

In this system diagram, we have color coded where in our code each part happens. The left-most column are all the steps that we go through in our main loop. For steps with additional rectangles to the right, these rectangles provide more information based on how we're going about performing those steps. 

When we run our main loop, we first initialize our reinforcement learning and neural network objects. Then we initialize our game. 

Next we get the action we should perform from the reinforcement learning algorithm given our game state. This is done by getting all the expected values of each action given the stat from our neural network. Then, the reinforcement learning algorithm picks and action. In some cases, this will be the best action with the highest predicted value, but in other cases, we might be exploratory and choose an action that we don't believe is quite as valuable to see what happens. 

After we perform the action, we see what the reward of performing that action was. 

Next we store the state action reward and new state tuple. Using our reinforcement learning algorithm, we convert this into a state, action, value, retard, new state, new action, predicted value tuple, and store this. 

Finally we train the reinforcement learning algorithm, by updating the value of our state using the following equation that we described in the reinforcement learning section:

$$Q_{t+1}(s_t,a_t) = Q_t(s_t, a_t) + \alpha_t(s_t,a_t)\times (R_{t+1} + \gamma \times max(Q_t(s_t+1,a)) - Q_t(s_t, a_t))$$

Then, we pass this value into our neural net as another example of what the value function ought to output and train the neural network again. 

*To see how we've actually written the code to perform each of these steps, please consult GridLearning/mainLoop.py, GridLearning/game.py, GridLearning/rl.py, or GridLearning/nn.py.*


## Our Explorations

### Learning to play 1 game
  In order to confirm that our reinforcement learning system could in fact learn we choose to test it out on learning a single board configuration. The board we started to train on looked like this:

  ``` python
   0 -1 -1
  -1 -1 -1
  -1  1 -1
  ```

  From the board above, we can see that the optimal number of moves to eat the food is 3. Here are our training results:

  ![](images/sameGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
    _The above shows the training results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on the same board every time._

  Our training results graph shows the number of moves it took the bot to eat all the food in every game it played while training. We also show the running average for the last 100 games. In the results above we see that it learned to eat the food in 3 moves after only 6 games. Pretty Good! Now let's see how it performs on a larger board. Here we tested it on a 4 by 4 board:

  ``` python
   0 -1 -1 -1
  -1 -1 -1  1
  -1 -1 -1 -1
  -1 -1 -1 -1
  ```
  We can see that the optimal number of moves to eat the food is 4. Here are our training results:

  ![](images/sameGame-4x4_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
     _The above shows the training results for a 4x4 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on the same board every time._

  It definitely took longer to learn to eat the food on the 4 by 4 board but it did learn to eat the food in 4 moves in less than 100 games. It's also worth noting in these graphs that we have exploratory behavior turned on so that the bot can learn how to play the game in the first place. Later on, we'll show graphs with the exploratory behavior turned off.

### Different game every n games 
  Before diving straight into a different board every game we choose to examine what would happen if we switched the boards every 50 games. The below figure shows our evaluation graph for this scenario. Our evaluation graphs were created by testing the bot on every possible board every 50 games and disabling exploratory behavior for the evaluation period. For example in the 3 by 3 case with 1 food there are 8 possible boards, in the 3 by 3 case with 2 foods there are 28 possible boards. For each board we allowed the bot 100 moves to eat all the food. If it hadn't completed the game in that length of time we knew it was impossible (on this size board) for it to complete the game. This is because in these cases, the bot hasn't learned how to play this particular game, or hasn't generalized and winds up moving back and forth indefinitely. We then plot the mean number of moves it took that bot to complete all the possible boards for each exploratory period. 

  ![](images/differentGameEvery50-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")
    _The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on a different board every 50 games._

  What we see here is that it's not really doing any generalized learning. The mean number of moves to complete all the different boards is not decreasing overall. However, when we look at the training results we see a more nuanced story.

  ![](images/differentGameEvery50-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
    _The above shows the training results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on a different board every 50 games._

  For the most part we see that there is a large spike in the number of moves played every 50 games (when the board changes) but it then quickly adapts to the new board state. This means that showing it the same board many times in a row leads it to optimize only for that board.

### Different game every time 
  Finally, we tried our system out on a different board every game. 

  ![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")
    _The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on a different board every game._

  In the above graph we can see that it is learning to eat the food better and better the more games is plays. Toward the end it is even playing the games perfectly much of the time. Let's take a look at our training graph.

  ![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-training.png "")
    _The above shows the training results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2 running on a different board every game._

  Similarly to our evaluation graph, we see that the bot improves over time. We also see that the bot is taking less time to complete a game even when it is not playing optimally. This is fantastic! From this point on we decided to focus on playing on a different board every game.

###Action Selection
#### $\epsilon$-greedy
One method of choosing which action to take is known as $\epsilon$-greedy, which selects the action with the highest value with probability $(1 - \epsilon)$ and a random action with probability $\epsilon$.
We explored different values of $\epsilon$, or exploratory rate, both constant and changing over time.

![](images/differentGame-3x3_1-constDF_0.5-greedy-constER_0.2-constLR_0.9-eval.png "")

_The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate of 0.2._

![](images/differentGame-3x3_1-constDF_0.5-greedy-decayER_0.5_5000-constLR_0.9-eval.png "")

 _The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate decaying from 0.5 with a time constant of 5000 games._

Using a fixed discount factor and learning rate, the best constant exploratory rate we found was 0.2. This means that each time we make a move (unless we're evaluating), there is a 20% chance of taking a random action instead of choosing the action with the greatest value predicted by the neural network.
We also looked at decreasing the exploratory rate each game using an exponential decay function.  The best parameters we found for this were a starting value of 0.5 and a time constant of 5000 games.  This appears to be better than the constant exploratory rate; it learns optimal play faster using the decaying exploratory rate.

#### Soft-max
  
Another method of action selection that we tried uses a soft-max with a Boltzmann distribution. This chooses the action with probabilities of $(e^{Q[s,a]/T})/(\sum_{a} e^{Q[s,a]/T})$ where $Q[s,a]$ is the value for a given state-action pair and $T$ is the "temperature", or how randomly it should choose actions. A higher value of $T$ will lead the bot to be more exploratory, while a lower value of $T$ will cause the bot to always select the best action.

We explored different values of $T$, both constant and changing over time.

![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.9-eval.png "")

_The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and temperature of 0.2._

![](images/differentGame-3x3_1-constDF_0.5-Boltz-decayER_0.2_2000-constLR_0.9-eval.png "")

_The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and temperature decaying from 0.2 with a time constant of 2000 games._

Using a fixed discount factor and learning rate, the best constant temperature we found was also 0.2. Also like in $\epsilon$-greedy, using an exponential decay function was better than holding the temperature constant. Here, a starting value of 0.5 and a time constant of 2000 games learned the fastest.
Something worth noting is that changing the starting value to 0.19 or 0.21 caused it to get stuck during a game a few thousand games in. We think this is due to the temperature dropping too low and not allowing it to move randomly when it needs to explore more. It appears that we were lucky with the starting value of 0.2.

#### Conclusion

Overall, it seems that the soft-max action selection is better than the $\epsilon$-greedy method. It learns faster and better, having smaller spikes later on when not evaluating (shown below, note the ranges of the y-axes).

![](images/differentGame-3x3_1-constDF_0.5-greedy-decayER_0.5_5000-constLR_0.9-training.png "")

_The above shows the training results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and exploratory rate decaying from 0.5 with a time constant of 5000 games._

![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.9-training.png "")

_The above shows the training results for a 3x3 grid with 1 food, discount factor of 0.5, learning rate of 0.9, and temperature of 0.2._

### Constant vs. Decaying Learning Rate
To start exploring the effect of varying the learning rate we tested a variety of constant learning rates from .01 to 1. The learning rate is a neural network parameter that determines how quickly, or slowly, we adjust the learning rate. Higher learning rates adjust the weights faster, while lower learning rates adjust the weights more slowly. Adjusting the learning rate allows us to ensure that we are learning how to play games as quickly as possible, without overfitting to each new game. We found that for the 3 by 3 board with 1 food the optimal constant learning rate was .1. The results from this test are shown below.
![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-constLR_0.1-eval.png "")
_The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, temperature of 0.2, and a learning rate of 0.1._

The results from this test were decent. It almost entirely stopped making errors by around 6500 games. Similarly to the testing we did on constant learning rates, we tried decaying the learning rate. The best results we had with a decaying learning rate are shown below.

![](images/differentGame-3x3_1-constDF_0.5-Boltz-constER_0.2-decayLR_0.4_2000-eval.png "")
_The above shows the evaluation results for a 3x3 grid with 1 food, discount factor of 0.5, temperature of 0.2, and a decaying initial learning rate of .4 with a time constant of 2000._

The best parameters with a decaying learning rate allowed for the bot to learn faster initially but it appears that it the constant learning rate allowed the bot to reach consistently optimal performance earlier.

### Other game spaces (Multiple food)
At the very end we were curious how it would preform with multiple food. The results indicate it does learn but does not reach optimal performance. Although it should be noted that we did not setup our system to optimize for the best route to eat all the food, but rather only how it can eat a single food the fastest. 
![](images/sameGame-3x3_3-constDF_0.5-Boltz-constER_0.2-constLR_0.2-eval.png "")
_The above shows the evaluation results for a 3x3 grid with 3 food, discount factor of 0.5, temperature of 0.2, and a learning rate of 0.2._

![](images/sameGame-3x3_3-constDF_0.5-Boltz-constER_0.2-constLR_0.2-training.png "")
_The above shows the training results for a 3x3 grid with 3 food, discount factor of 0.5, temperature of 0.2, and a learning rate of 0.2._

## Conclusions
The Explorations we presetned above are some explorations we did when investigating how tweaking some of our reinforcement learning/neural network parameters. These explorations of the parameters we did tweak are by no means exhaustive, but they did help us to gain an intuition for how reinforcement learning and neural networks could be used together to teach a computer how to do something. Although our game, for most of our explorations, is relatively simple, this game could, given enough time, likely be extended fairly easily and make for an interesting path planning scenario. If we were to conintue working on this project, we would continue to do a more thurough sweep of all the parameters we explored above and perhaps transition to a more complicated game-space. 

## Sources
Here is a list of some of the sources we found very helpful as we were doing this project!

* [“Reinforcement Learning: An Introduction", by Richard Sutton and Andrew Barto](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)

* [The Ogar Repository](https://github.com/OgarProject/Ogar)

* [An implementation that uses TensorFlow (the neural network library we used) for reinforcement leargning)](https://github.com/nivwusquorum/tensorflow-deepq)

* [A paper on Q-Learning with Neural Networks](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1526924)

* [Another paper on Q-Learning with Neural Networks](https://www.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf)

* [A video explaining neural networks](https://www.youtube.com/watch?v=bH6VnezBZfI)

* [Slides from a presentation about reinforcement learning and neural networks](http://web.mst.edu/~gosavia/neural_networks_RL.pdf)




</xmp>
<script src="http://cdn.ztx.io/strapdown/strapdown.min.js"></script>
</html>
